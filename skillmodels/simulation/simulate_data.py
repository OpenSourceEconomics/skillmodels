"""Functions to simulate a dataset generated by a latent factor model.

Notes:
    - I use abbreviations to describe the sizes of arrays. An overview is here:
        https://skillmodels.readthedocs.io/en/latest/names_and_concepts.html
    - what is called factors here is the same as states in the assignments.
    - You can use additional functions if you want. Their name should start
        with an underscore to make clear that those functions should not be
        used in any other module.
    - Please write tests for all functions except simulate_dataset.
        I know that functions involving randomness are hard to test. The
        best way is to replace (patch) the methods that actually generate
        random numbers with a so called mock function while testing. It can
        be done with this library:
        https://docs.python.org/3/library/unittest.mock.html
        I do similar stuff in many places of skillmodels but it is quite difficult,
        so you can also ask me once you get there and we can do it together.
    - The tests should be in a module in
        `skillmodels/tests/simulation/simulate_dataset_test.py.
    - Use pytest for the tests (as you learned in the lecture) even though
        the other tests in skillmodels use an older library
    - I added some import statements but you will probably need more
    - Please delete all notes in the docstrings when you are done.
    - It is very likely that I made some mistakes in the docstrings or forgot an
        argument somewhere. Just send me an email in that case or come to my office.
"""
import sys
import pandas as pd
import numpy as np
from numpy.random import multivariate_normal, choice, binomial

sys.path.append("../")
import model_functions.transition_functions as tf


def add_missings(data, meas_names, p_b, p_r):
    """Add np.nans to data.

    nans are only added to measurements, not to control variables or factors.

    The function does not modify data in place. (create a new one)

    Args:
        data (pd.DataFrame): contains the observable part of a simulated dataset
        meas_names (list): list of strings of names of each measurement variable
        p_b (float): probability of a measurement to become missing
        p_r (float): probability of a measurement to remain missing in the next period
    Returns:
        data_with_missings (pd.DataFrame): Dataset with a share of measurements
        replaced by np.nan values

    Notes:
        - Time_periods should be sorted for each individual
        - p is NOT the marginal probability of a measurement being missing.
          The marginal probability is given by: p_m = p/(1-serial_corr), where
          serial_corr = (p_r-p_b) in general != 0, since p_r != p_b. This means that in
          average the share of missing values (in the entire dataset) will be larger
          than p. Thus, p and q should be set accordingly given the desired share
          of missing values.
        - I would still suggest to draw period_0 bernoulli with the marginal
          probaility. Having run the function in a loop with 100 iteration, for
          100 pairs of p_r and p_b , the average share of missing measurements in all
          measurements is very close to p_b/(1-(p_r-p_b)) (the average percentage
          deviation is less than 2 %). Sounds like a nice result and a simple
          formula for deciding on p_b and p_r.
    """

    nmeas = len(meas_names)
    data_with_missings = data.copy(deep=True)
    for i in set(data_with_missings.index):
        ind_data = data_with_missings.loc[i][meas_names].values
        s_0 = binomial(1, p_b, nmeas)
        ind_data[0, np.where(s_0 == 1)] = np.nan
        for t in range(1, len(ind_data)):
            indc_nan = np.isnan(ind_data[t - 1])
            prob = p_r * indc_nan + p_b * (1 - indc_nan)
            s_m = binomial(1, prob)
            ind_data[t, np.where(s_m == 1)] = np.nan
        data_with_missings.loc[i, meas_names] = ind_data

    return data_with_missings


def simulate_datasets(
    factor_names,
    control_names,
    meas_names,
    nobs,
    nper,
    transition_names,
    transition_argument_dicts,
    shock_variances,
    loadings,
    deltas,
    meas_variances,
    means,
    covs,
    weights=1,
):
    """Simulate datasets generated by a latent factor model.

    This function calls the remaining functions in this module.

    Args:
         nper (int): number of time periods the dataset contains
         nobs (int): number of observations
         factor_names (list): list of strings of names of each factor
         control_names (list): list of strings of names of each control variable
         meas_names (list): list of strings of names of each measurement variable
         loadings (np.ndarray): numpy array of size (nmeas, nfac)
         deltas (np.ndarray): numpy array of size (nmeas, ncontrols)
         transition_names (list): list of strings with the names of the transition
            function of each factor.
         transition_argument_dicts (list): list of dictionaries of length nfac with
            the arguments for the transition function of each factor. A detailed description
            of the arguments of transition functions can be found in the module docstring
            of skillmodels.model_functions.transition_functions.
         shock_variances (np.ndarray): numpy array of length nfac.
         meas_variances (np.ndarray): numpy array of size (nmeas) with the variances of the
            measurements. Measurement error is assumed to be independent across measurements
         means (np.ndarray): size (nemf, nfac + ncontrols)
         covs (np.ndarray): size (nemf, nfac + ncontrols, nfac + ncontrols)
         weights (np.ndarray): size (nemf). The weight of each mixture element.
               The default value is 1.

    Returns:
        observed_data (pd.DataFrame): Dataset with measurements and control variables
            in long format
        latent_data (pd.DataFrame): Dataset with lantent factors in long format
    """
    ncont = len(control_names)
    nfac = len(factor_names)
    fac = [np.zeros((nobs, nfac))] * nper
    # array of id_s repeated n_per times
    obs_id = np.tile(np.arange(nobs), nper)

    fac[0], cont = generate_start_factors_and_control_variables(
        nobs, nfac, ncont, means, covs, weights
    )

    cont = pd.DataFrame(
        data=np.array([cont] * nper).reshape(nobs * nper, ncont),
        columns=control_names,
        index=obs_id,
    )

    for i in range(1, nper):
        fac[i] = next_period_factors(
            fac[i - 1], transition_names, transition_argument_dicts, shock_variances
        )

    meas = pd.DataFrame(
        data=measurements_from_factors(
            np.array(fac).reshape(nobs * nper, nfac),
            cont.values,
            loadings,
            deltas,
            meas_variances,
        ),
        columns=meas_names,
        index=obs_id,
    )

    t_col = pd.DataFrame(
        np.repeat(range(nper), nobs), columns=["time_period"], index=obs_id
    )

    observed_data = pd.concat([t_col, meas, cont], axis=1)
    latent_data = pd.DataFrame(
        data=np.array(fac).reshape(nobs * nper, nfac),
        columns=factor_names,
        index=obs_id,
    )

    latent_data = pd.concat([t_col, latent_data], axis=1)

    return observed_data, latent_data


def generate_start_factors_and_control_variables(
    nobs, nfac, ncont, means, covs, weights=1
):
    """Draw initial states and control variables from a (mixture of) normals.

    Args:
        nobs (int): number of observations
        nfac (int): number of factor (latent) variables
        ncont (int): number of control variables
        means (np.ndarray): size (nemf, nfac + ncontrols)
        covs (np.ndarray): size (nemf, nfac + ncontrols, nfac + ncontrols)
        weights (np.ndarray): size (nemf). The weight of each mixture element.
                              Default value is equal to 1.


    Returns:
        start_factors (np.ndarray): shape (nobs, nfac),
        controls (np.ndarray): shape (nobs, ncontrols),
    """

    if np.size(weights) == 1:
        out = multivariate_normal(means, covs, nobs)
    else:
        helper_array = choice(np.arange(len(weights)), p=weights, size=nobs)
        out = np.zeros((nobs, nfac + ncont))
        for i in range(nobs):
            out[i] = multivariate_normal(means[helper_array[i]], covs[helper_array[i]])
    start_factors = out[:, 0:nfac]
    controls = out[:, nfac:]

    return start_factors, controls


def next_period_factors(
    factors, transition_names, transition_argument_dicts, shock_variances
):
    """Apply transition function to factors and add shocks.

    Args:
        factors (np.ndarray): shape (nobs, nfac)
        transition_names (list): list of strings with the names of the transition
            function of each factor.
        transition_argument_dicts (list): list of dictionaries of length nfac with
            the arguments for the transition function of each factor. A detailed description
            of the arguments of transition functions can be found in the module docstring
            of skillmodels.model_functions.transition_functions.
        shock_variances (np.ndarray): numpy array of length nfac.

    Returns:
        next_factors (np.ndarray): shape(nobs,nfac)
    """
    nobs, nfac = factors.shape
    # sigma_points = factors
    factors_tp1 = np.zeros((nobs, nfac))
    for i in range(nfac):
        factors_tp1[:, i] = getattr(tf, transition_names[i])(
            factors, **transition_argument_dicts[i]
        )
    # Assumption: In general err_{Obs_j,Fac_i}!=err{Obs_k,Fac_i}, where j!=k
    errors = multivariate_normal([0] * nfac, np.diag(shock_variances), nobs).reshape(
        nobs, nfac
    )
    next_factors = factors_tp1 + errors

    return next_factors


def measurements_from_factors(factors, controls, loadings, deltas, variances):
    """Generate the variables that would be observed in practice.

    This generates the data for only one period. Let nmeas be the number
    of measurements in that period.

    Args:
        factors (pd.DataFrame or np.ndarray): DataFrame of shape (nobs, nfac)
        controls (pd.DataFrame or np.ndarray): DataFrame of shape (nobs, ncontrols)
        loadings (np.ndarray): numpy array of size (nmeas, nfac)
        deltas (np.ndarray): numpy array of size (nmeas, ncontrols)
        variances (np.ndarray): numpy array of size (nmeas) with the variances of the
            measurements. Measurement error is assumed to be independent across measurements

    Returns:
        measurements (np.ndarray): array of shape (nobs, nmeas) with measurements.
    """
    nmeas = loadings.shape[0]
    nobs, nfac = factors.shape
    ncontrols = controls.shape[1]
    # Assumption: In general eps_{Obs_j,Meas_i}!=eps_{Obs_k,Meas_i}  where j!=k
    epsilon = multivariate_normal([0] * nmeas, np.diag(variances), nobs).reshape(
        nobs, 1, nmeas
    )
    states = factors.reshape(nobs, 1, nfac)
    conts = controls.reshape(nobs, 1, ncontrols)
    meas = np.dot(states, loadings.T) + np.dot(conts, deltas.T) + epsilon
    return meas.reshape(nobs, nmeas)


def _mv_student_t(mean, cov, d_f, size=1):
    """Generate random sample from d-dimensional t_distribution
    Args:
        mu (np.ndarray): vector of mean of size d
        cov (np.ndarray): covariance matrix of shape (d,d)
        d_f (float): degree of freedom
        size (float): the sample size
    Returns:
        mv_t(np.ndarray): shape (size, d)
    Notes:
       - Cov is the variance-covariance of the generated random variable.
       - So that the normal vector is to be generated from N(0,cov*(nu-2)/nu)
       - Ref: https://bit.ly/2NDhbWM

    """
    d_dim = len(cov)
    x_chi = np.sqrt(np.random.chisquare(d_f, size) / d_f).reshape(size, 1, 1)
    y_norm = multivariate_normal(np.zeros(d_dim), cov * (d_f - 2) / d_f, size).reshape(
        size, 1, d_dim
    )
    mv_t = mean + (y_norm / x_chi).reshape(size, d_dim)
    return mv_t


def _uv_elip_stable(delta, gamma, alpha, beta=1,size=1):
    """An algorithm to for simulating random variables from (symmetric) stable 
    
    distribution.
    Args:
        alpha (float): measure of concentration
        beta (float): measure of skewness.
        delta (float): location parameter
        gamma (float): scale parameter
    Returns:
        s_return (float): S(alpha, beta, gamm, delta) random variable
    Notes:
       
       - ref: Chambers et al.(1976) , Nolan (2018)
       - This is the general case. For the purpose of generating form
         a multivariate elliptically countered stabel rv would suffice 
         to set beta==1 and restrict alpha<1 (strictly).
    
    """
    theta=np.random.uniform(-np.pi/2, np.pi/2,size) #!!!!!!Need to check!!!!!!!!!
    w_exp = np.random.exponential(1,size)
    if alpha==1:
        comp_1 = np.tan(theta)*(0.5*np.pi+beta-theta)
        comp_2 = -beta*np.log(0.5*w_exp*np.cos(theta)/(0.5*np.pi+beta*theta))
        zeta = 2/np.pi*(comp_1+comp_2)
        s_return = gamma*zeta+(delta+beta*0.5*np.pi*np.log(gamma))
    else:
        theta_0 = np.arctan(beta*np.tan(0.5*np.pi*alpha))/alpha
        z_1 = np.sin(alpha*(theta_0+theta))/((np.cos(theta)*np.cos(alpha/theta_0))^(1/alpha))
        z_2 = (np.cos((alpha-1)*theta+alpha*theta_0)/w_exp)^(1/alpha-1)
        zeta = z_1*z_2
        s_return = gamma*zeta+delta


    return s_return

def _mv_elip_stable(alpha, sigma_mat, delta,size=1):
     """An algorithm to generate d-dimensional multivariate elliptically contoured stable rv
     Args:
        alpha (float): measure of concentration strictly between 0 and 2
        sigma_mat (np.ndarray): positive definite matrix of shape (d,d)
        delta (np.ndarray): shift vector of size d
     Returns:
        mv_stab (np.ndarray): rv of shape (size, d)
     Notes:
       - ref: Nolan (2013)
     """

        a_stab = _uv_elip_stable(0, 2*(np.cos(np.pi*alpha/4))^(2/alpha),0.5*alpha, beta=1, size=size)
        g_norm = multivariate_normal(np.zeros(len(sigma_mat)),sigma_mat,size)
        mv_stab = np.sqrt(a_stab)*g_norm+delta #need to check the shapes!!!!!!!

     return mv_stab
