"""Functions to simulate a dataset generated by a latent factor model.

Notes:
    - I use abbreviations to describe the sizes of arrays. An overview is here:
        https://skillmodels.readthedocs.io/en/latest/names_and_concepts.html
    - what is called factors here is the same as states in the assignments.
    - You can use additional functions if you want. Their name should start
        with an underscore to make clear that those functions should not be
        used in any other module.
    - Please write tests for all functions except simulate_dataset.
        I know that functions involving randomness are hard to test. The
        best way is to replace (patch) the methods that actually generate
        random numbers with a so called mock function while testing. It can
        be done with this library:
        https://docs.python.org/3/library/unittest.mock.html
        I do similar stuff in many places of skillmodels but it is quite difficult,
        so you can also ask me once you get there and we can do it together.
    - The tests should be in a module in
        `skillmodels/tests/simulation/simulate_dataset_test.py.
    - Use pytest for the tests (as you learned in the lecture) even though
        the other tests in skillmodels use an older library
    - I added some import statements but you will probably need more
    - Please delete all notes in the docstrings when you are done.
    - It is very likely that I made some mistakes in the docstrings or forgot an
        argument somewhere. Just send me an email in that case or come to my office.

"""

import pandas as pd
import numpy as np
from numpy.random import multivariate_normal, choice
import sys

sys.path.append("../model_functions/")
import transition_functions as tf

# import skillmodels.model_functions.transition_functions as tf


def simulate_datasets(
    factor_names,
    control_names,
    meas_names,
    nobs,
    nper,
    means,
    covs,
    weights,
    transition_names,
    transition_argument_dicts,
    shock_variances,
    loadings,
    deltas,
    meas_variances,
):
    """Simulate datasets generated by a latent factor model.

    This function calls the remaining functions in this module.

    Implement this function at the very end and only after I accepted your pull
    request for the remaining functions. You can then either figure out a suitable
    list of arguments yourself or ask me again.
    
    Args:
         nper (int): number of time periods the dataset contains
         nobs (int): number of observations
         factor_names (list): list of strings of names of each factor
         control_names (list): list of strings of names of each control variable
         meas_names (list): list of strings of names of each measurement variable
         means (np.ndarray): size (nemf, nfac + ncontrols)
         covs (np.ndarray): size (nemf, nfac + ncontrols, nfac + ncontrols)
         weights (np.ndarray): size (nemf). The weight of each mixture element
         loadings (np.ndarray): numpy array of size (nmeas, nfac)
         deltas (np.ndarray): numpy array of size (nmeas, ncontrols)
         transition_names (list): list of strings with the names of the transition
            function of each factor.
         transition_argument_dicts (list): list of dictionaries of length nfac with
            the arguments for the transition function of each factor. A detailed description
            of the arguments of transition functions can be found in the module docstring
            of skillmodels.model_functions.transition_functions.
         shock_variances (np.ndarray): numpy array of length nfac.
         meas_variances (np.ndarray): numpy array of size (nmeas) with the variances of the
            measurements. Measurement error is assumed to be independent across measurements


    Returns:
        observed_data (pd.DataFrame): Dataset with measurements and control variables
            in long format
        latent_data (pd.DataFrame): Dataset with lantent factors in long format
    """
    ncont = len(control_names)
    nfac = len(factor_names)
    nmeas=len(meas_names)
    out_fac = [np.zeros((nobs, nfac))] * nper
    out_id = np.array([range(nobs)] * nper).reshape(
        nobs * nper
    )  # array of id_s repeated n_per times
    out_fac[0], out_cont = generate_start_factors_and_control_variables(
        means, covs, weights, nobs, nfac, ncont
    )
    out_cont = pd.DataFrame(
        data=np.array([out_cont] * nper).reshape(nobs * nper, ncont),
        columns=control_names,
        index=out_id,
    )

    for i in range(1, nper):
        out_fac[i] = next_period_factors(
            out_fac[i - 1], transition_names, transition_argument_dicts, shock_variances
        )
    out_meas = pd.DataFrame(
        data=measurements_from_factors(
            np.array(out_fac).reshape(nobs * nper, nfac),
            out_cont.values,
            loadings,
            deltas,
            meas_variances,
            nmeas,
        ),
        columns=meas_names,
        index=out_id,
    )
    out_t = pd.DataFrame(
        np.repeat(range(nper), nobs), columns=["time_period"], index=out_id
    )
    observed_data = pd.concat([out_t, out_meas, out_cont], axis=1)
    latent_data = pd.DataFrame(
        data=np.array(out_fac).reshape(nobs * nper, nfac),
        columns=factor_names,
        index=out_id,
    )
    latent_data = pd.concat([out_t, latent_data], axis=1)

    return observed_data, latent_data


def generate_start_factors_and_control_variables(
    means, covs, weights, nobs, nfac, ncont
):
    """Draw initial states and control variables from a (mixture of) normals.

    Args:
        means (np.ndarray): size (nemf, nfac + ncontrols)
        covs (np.ndarray): size (nemf, nfac + ncontrols, nfac + ncontrols)
        weights (np.ndarray): size (nemf). The weight of each mixture element.
        nobs (int): number of observations
        nfac (int): number of factor (latent) variables
        ncont (int): number of control variables

    Returns:
        start_factors (np.ndarray): shape (nobs, nfac),
        controls (np.ndarray): shape (nobs, ncontrols),

    Notes:
        In the long run I would like to generalize this to drawing from a mixture of
        elliptical distributions: https://en.wikipedia.org/wiki/Elliptical_distribution
        This contains the multivariate normal as a special case.
        It would require an interface change because the elliptical distribution has more
        parameters than just mean and covariance. It would be great if you make a proposal
        for this general case.

    """

    if np.size(weights) != 1:
        helper_array = choice(np.arange(len(weights)), p=weights, size=nobs)
        out = np.zeros((nobs, nfac + ncont))
        for i in range(nobs):
            out[i] = multivariate_normal(means[helper_array[i]], covs[helper_array[i]])
    else:
        out = multivariate_normal(means, covs, nobs)
    start_factors = out[:, 0:nfac]
    controls = out[:, nfac:]

    return start_factors, controls


def next_period_factors(
    factors, transition_names, transition_argument_dicts, shock_variances
):
    """Apply transition function to factors and add shocks.

    Args:
        factors (np.ndarray): shape (nobs, nfac)
        transition_names (list): list of strings with the names of the transition
            function of each factor.
        transition_argument_dicts (list): list of dictionaries of length nfac with
            the arguments for the transition function of each factor. A detailed description
            of the arguments of transition functions can be found in the module docstring
            of skillmodels.model_functions.transition_functions.
        shock_variances (np.ndarray): numpy array of length nfac.

    Returns:
        next_factors (np.ndarray): shape(nobs,nfac)

    Notes:
        - You can look at the module `transform_sigma_points` to see how you can use
        getattr() to call the transition functions based on their name

        - Writing this function is quite complex because it reuses a lot of code for
             the transition functions. Take the time to read the documentation of those
             functions if you feel it is necessary

        - The shocks for the different factors are assumed to be independent. You can draw
            them from a multivariate normal with diagonal covariance matrix or from
            nfac univariate normals.

        - You have to convert the factors to a numpy array (DataFrame.values) and then convert
            the result back in the end. For speed reasons all the transition functions
            expect numpy arrays and not pandas DataFrames.


    """
    nobs, nfac = factors.shape
    # sigma_points = factors
    factors_tp1 = np.zeros((nobs, nfac))
    for i in range(nfac):
        factors_tp1[:, i] = getattr(tf, transition_names[i])(
            factors, **transition_argument_dicts[i]
        )
    # Assumption: In general err_{Obs_j,Fac_i}!=err{Obs_k,Fac_i}, where j!=k
    errors = multivariate_normal([0] * nfac, np.diag(shock_variances), nobs).reshape(
        nobs, nfac
    )
    next_factors = factors_tp1 + errors
    # next_factors = pd.DataFrame(data = factors_tp1, columns = factors.columns)

    return next_factors


def measurements_from_factors(
    factors, controls, loadings, deltas, variances, nmeas
):
    """Generate the variables that would be observed in practice.

    This generates the data for only one period. Let nmeas be the number of measurements in that period.

    Args:
        factors (pd.DataFrame or np.ndarray): DataFrame of shape (nobs, nfac)
        controls (pd.DataFrame or np.ndarray): DataFrame of shape (nobs, ncontrols)
        loadings (np.ndarray): numpy array of size (nmeas, nfac)
        deltas (np.ndarray): numpy array of size (nmeas, ncontrols)
        variances (np.ndarray): numpy array of size (nmeas) with the variances of the
            measurements. Measurement error is assumed to be independent across measurements
        instead: measurement_names (list): list of length nmeas with the names of the measurements
        read: nmeas (int): number of measurments 

    Returns:
        measurements (pd.DataFrame): DataFrame of shape (nobs, nmeas) with measurement
            names as columns.

    Notes:
        - A measurement y is a linear function of latent factors and control variables, i.e.
            y = factors times loadings + controls times deltas + epsilon
            This is a slide extension of the measurement model you know from the assignments.
        - Try to express as much as possible in matrix products. This will lead to concise and
            fast code.
    """
    nobs, nfac = factors.shape
    ncontrols = controls.shape[1]
    # Assumption: In general eps_{Obs_j,Meas_i}!=eps_{Obs_k,Meas_i}  where j!=k
    epsilon = multivariate_normal([0] * nmeas, np.diag(variances), nobs).reshape(
        nobs, 1, nmeas
    )
    states = factors.reshape(nobs, 1, nfac)
    conts = controls.reshape(nobs, 1, ncontrols)
    meas = np.dot(states, loadings.T) + np.dot(conts, deltas.T) + epsilon
    # measurements = pd.DataFrame(data = meas.reshape(nobs,nmeas),columns = measurement_names)

    return meas.reshape(nobs, nmeas)
