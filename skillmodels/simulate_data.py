"""Functions to simulate a dataset generated by a latent factor model."""
import warnings

import jax.numpy as jnp
import numpy as np
import pandas as pd
from numpy.random import choice
from numpy.random import multivariate_normal

from skillmodels.filtered_states import anchor_states_df
from skillmodels.kalman_filters import transform_sigma_points
from skillmodels.params_index import get_params_index
from skillmodels.parse_params import create_parsing_info
from skillmodels.parse_params import parse_params
from skillmodels.process_data import process_data
from skillmodels.process_debug_data import create_state_ranges
from skillmodels.process_model import process_model


def simulate_dataset(model_dict, params, n_obs=None, data=None, policies=None):

    """Simulate datasets generated by a latent factor model.

    Args:
        model_dict (dict): The model specification. See: :ref:`model_specs`
        params (pandas.DataFrame): DataFrame with model parameters.
        n_obs (int): Number of simulated individuals
        data (pd.DataFrame): Dataset in the same format as for estimation, containing
            information about observed factors and control variables.
        policies (list): list of dictionaries. Each dictionary specifies a
            a stochastic shock to a latent factor AT THE END of "period" for "factor"
            with mean "effect_size" and "standard deviation"

    Returns:
        observed_data (pd.DataFrame): Dataset with measurements and control variables
            in long format

        latent_data (pd.DataFrame): Dataset with latent factors in long format

    """
    if data is None and n_obs is None:
        raise ValueError("If data is None, n_obs has to be provided.")

    model = process_model(model_dict)

    if model["labels"]["observed_factors"] and data is None:
        raise ValueError(
            "To simulate a model with observed factors, data cannot be None."
        )

    if model["labels"]["controls"] != ["constant"] and data is None:
        raise ValueError("To simulate a model with controls, data cannot be None.")

    if data is not None:
        control_data, observed_data = process_data(
            df=data,
            labels=model["labels"],
            update_info=model["update_info"],
            anchoring_info=model["anchoring"],
            purpose="simulation",
        )
        data_n_obs = control_data.shape[1]

        if n_obs is not None and data_n_obs != n_obs:
            warnings.warn(
                f"The number of observations inferred from data ({data_n_obs}) and "
                f"n_obs ({n_obs}) are different. n_obs is ignored."
            )
        n_obs = data_n_obs

    else:
        control_data = jnp.ones((n_obs, 1))
        n_periods = model["dimensions"]["n_periods"]
        observed_data = jnp.zeros((n_periods, n_obs, 0))

    params_index = get_params_index(
        update_info=model["update_info"],
        labels=model["labels"],
        dimensions=model["dimensions"],
        transition_info=model["transition_info"],
    )

    params = params.reindex(params_index)

    parsing_info = create_parsing_info(
        params_index=params.index,
        update_info=model["update_info"],
        labels=model["labels"],
        anchoring=model["anchoring"],
    )

    states, covs, log_weights, pardict = parse_params(
        params=jnp.array(params["value"].to_numpy()),
        parsing_info=parsing_info,
        dimensions=model["dimensions"],
        labels=model["labels"],
        n_obs=n_obs,
    )

    observed_data, latent_data = _simulate_dataset(
        latent_states=states,
        covs=covs,
        log_weights=log_weights,
        pardict=pardict,
        labels=model["labels"],
        dimensions=model["dimensions"],
        n_obs=n_obs,
        update_info=model["update_info"],
        control_data=control_data,
        observed_factor_data=observed_data,
        policies=policies,
        transition_info=model["transition_info"],
    )

    anchored_latent_data = anchor_states_df(
        states_df=latent_data, model_dict=model_dict, params=params
    )

    out = {
        "unanchored_states": {
            "states": latent_data,
            "state_ranges": create_state_ranges(
                latent_data, model["labels"]["latent_factors"]
            ),
        },
        "anchored_states": {
            "states": anchored_latent_data,
            "state_ranges": create_state_ranges(
                anchored_latent_data, model["labels"]["latent_factors"]
            ),
        },
        "measurements": observed_data,
    }

    return out


def _simulate_dataset(
    latent_states,
    covs,
    log_weights,
    pardict,
    labels,
    dimensions,
    n_obs,
    update_info,
    control_data,
    observed_factor_data,
    policies,
    transition_info,
):
    """Simulate datasets generated by a latent factor model.

    Args:
        See simulate_data

    Returns:
        See simulate_data

    """
    policies = policies if policies is not None else []

    n_states = dimensions["n_latent_factors"]
    n_periods = dimensions["n_periods"]

    weights = np.exp(log_weights)[0]
    loadings_df = pd.DataFrame(
        data=pardict["loadings"],
        index=update_info.index,
        columns=labels["latent_factors"],
    )

    control_params_df = pd.DataFrame(
        data=pardict["controls"], index=update_info.index, columns=labels["controls"]
    )
    meas_sds = pd.DataFrame(
        data=pardict["meas_sds"].reshape(-1, 1), index=update_info.index
    )

    transition_params = pardict["transition"]
    shock_sds = pardict["shock_sds"]

    dist_args = []
    for mixture in range(dimensions["n_mixtures"]):
        args = {
            "mean": latent_states[0][mixture],
            "cov": covs[0][mixture].T @ covs[0][mixture],
        }
        dist_args.append(args)

    latent_states = np.zeros((n_periods, n_obs, n_states))
    latent_states[0] = generate_start_states(n_obs, dimensions, dist_args, weights)

    for t in range(n_periods - 1):
        # if there is a shock in period t, add it here
        policies_t = [p for p in policies if p["period"] == t]
        for policy in policies_t:
            position = labels["latent_factors"].index(policy["factor"])
            latent_states[t, :, position] += _get_shock(
                mean=policy["effect_size"], sd=policy["standard_deviation"], size=n_obs
            )

        # get combined states and observed factors as jax array
        to_concat = [latent_states[t], observed_factor_data[t]]
        states = jnp.array(np.concatenate(to_concat, axis=-1))
        # reshaping is just needed for transform sigma points
        states = states.reshape(1, 1, *states.shape)

        # extract trans coeffs for the period
        trans_coeffs = {k: arr[t] for k, arr in transition_params.items()}

        # get anchoring_scaling_factors for the period
        anchoring_scaling_factors = pardict["anchoring_scaling_factors"][
            jnp.array([t, t + 1])
        ]
        # get anchoring constants for the period
        anchoring_constants = pardict["anchoring_constants"][jnp.array([t, t + 1])]

        # call transform_sigma_points and convert result to numpy
        next_states = np.array(
            transform_sigma_points(
                sigma_points=states,
                transition_info=transition_info,
                trans_coeffs=trans_coeffs,
                anchoring_scaling_factors=anchoring_scaling_factors,
                anchoring_constants=anchoring_constants,
            )
        ).reshape(n_obs, -1)

        errors = multivariate_normal(
            mean=np.zeros(n_states), cov=np.diag(shock_sds[t] ** 2), size=n_obs
        )
        next_states = next_states + errors

        latent_states[t + 1] = next_states

    observed_data_by_period = []

    for t in range(n_periods):
        meas = pd.DataFrame(
            data=measurements_from_states(
                latent_states[t],
                control_data[t],
                loadings_df.loc[t].to_numpy(),
                control_params_df.loc[t].to_numpy(),
                meas_sds.loc[t].to_numpy().flatten(),
            ),
            columns=loadings_df.loc[t].index,
        )
        meas["period"] = t
        observed_data_by_period.append(meas)

    observed_data = pd.concat(observed_data_by_period, axis=0, sort=True)
    observed_data["id"] = observed_data.index
    observed_data.sort_values(["id", "period"], inplace=True)

    latent_data_by_period = []
    for t in range(n_periods):
        lat = pd.DataFrame(data=latent_states[t], columns=labels["latent_factors"])
        lat["period"] = t
        latent_data_by_period.append(lat)

    latent_data = pd.concat(latent_data_by_period, axis=0, sort=True)
    latent_data["id"] = latent_data.index
    latent_data.sort_values(["id", "period"], inplace=True)

    return observed_data, latent_data


def _get_shock(mean, sd, size):
    """Add stochastic effect to a  factor of length n_obs.

    Args:
        mean (float): mean of the stochastic effect
        sd (float): standard deviation of the effect
        size (int): length of resulting array

    Returns:
        shock (np.array): 1d array of length n_obs with the stochastic shock

    """
    if sd == 0:
        shock = np.full(size, mean)
    elif sd > 0:
        shock = np.random.normal(mean, sd, size)
    else:
        raise ValueError("No negative standard deviation allowed.")
    return shock


def generate_start_states(n_obs, dimensions, dist_args, weights):
    """Draw initial states and control variables from a (mixture of) normals.

    Args:
        n_obs (int): number of observations
        dimensions (dict): Dimensional information like n_states, n_periods, n_controls,
            n_mixtures. See :ref:`dimensions`.
        dist_args (list): list of dicts of length nmixtures of dictionaries with the
            entries "mean" and "cov" for each mixture distribution.

    Returns:
        start_states (np.ndarray): shape (n_obs, n_states),
        controls (np.ndarray): shape (n_obs, n_controls),

    """
    n_states = dimensions["n_latent_factors"]
    if np.size(weights) == 1:
        out = multivariate_normal(size=n_obs, **dist_args[0])
    else:
        helper_array = choice(np.arange(len(weights)), p=weights, size=n_obs)
        out = np.zeros((n_obs, n_states))
        for i in range(n_obs):
            out[i] = multivariate_normal(**dist_args[helper_array[i]])

    return out


def measurements_from_states(states, controls, loadings, control_params, sds):
    """Generate the variables that would be observed in practice.

    This generates the data for only one period. Let n_meas be the number
    of measurements in that period.

    Args:
        states (pd.DataFrame or np.ndarray): DataFrame of shape (n_obs, n_states)
        controls (pd.DataFrame or np.ndarray): DataFrame of shape
            (n_obs, n_controlsrols)
        loadings (np.ndarray): numpy array of size (n_meas, n_states)
        control_coeffs (np.ndarray): numpy array of size (n_meas, n_states)
        sds (np.ndarray): numpy array of size (n_meas) with the standard deviations
            of the measurements. Measurement error is assumed to be independent
            across measurements.

    Returns:
        measurements (np.ndarray): array of shape (n_obs, n_meas) with measurements.

    """
    n_meas = loadings.shape[0]
    n_obs = len(states)
    epsilon = multivariate_normal([0] * n_meas, np.diag(sds ** 2), n_obs)
    states_part = np.dot(states, loadings.T)
    control_part = np.dot(controls, control_params.T)
    meas = states_part + control_part + epsilon
    return meas
